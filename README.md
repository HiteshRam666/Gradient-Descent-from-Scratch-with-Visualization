# Gradient-Descent-from-Scratch-with-Visualization

# WHAT IS GRADIENT DESCENT IN MACHINE LEARNING?

Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (AI) and computer science applications.

![631731_P7z2BKhd0R-9uyn9ThDasA](https://github.com/HiteshRam666/Gradient-Descent-from-Scratch-/assets/116026459/54a0df8c-38ac-41a9-8810-63207cdc8952)


## How does gradient descent work?

- **Learning rate** (also referred to as step size or the alpha) is the size of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High learning rates result in larger steps but risks overshooting the minimum. Conversely, a low learning rate has small step sizes. While it has the advantage of more precision, the number of iterations compromises overall efficiency as this takes more time and computations to reach the minimum.

- **The cost** (or loss) function measures the difference, or error, between actual y and predicted y at its current position. This improves the machine learning model's efficacy by providing feedback to the model so that it can adjust the parameters to minimize the error and find the local or global minimum. It continuously iterates, moving along the direction of steepest descent (or the negative gradient) until the cost function is close to or at zero. At this point, the model will stop learning. Additionally, while the terms, cost function and loss function, are considered synonymous, there is a slight difference between them. Itâ€™s worth noting that a loss function refers to the error of one training example, while a cost function calculates the average error across an entire training set.

  ![image](https://github.com/HiteshRam666/Gradient-Descent-from-Scratch-/assets/116026459/5d1c3123-9b48-456f-a572-88f744bf48e2)

  ## Visualizing Gradient Descent with Matplotlib

  ## Gradient Descent
  ![normal](https://github.com/HiteshRam666/Gradient-Descent-from-Scratch-with-Visualization/assets/116026459/a1970419-0706-4df0-a3ba-c42238bcf3b9)

  ## Sin-Cosine Wave
  ![sin_cos_wave](https://github.com/HiteshRam666/Gradient-Descent-from-Scratch-with-Visualization/assets/116026459/5ba20c5a-59d7-46ce-88b4-ad46d48e0b8e)

  ## 3D-Visualization
  

https://github.com/HiteshRam666/Gradient-Descent-from-Scratch-with-Visualization/assets/116026459/91b932bb-ebca-400f-abc2-96c6251b3343



